{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) tokenizer\n",
    "\n",
    "This tokenizers uses sub-words which are learned by observing the occurences of tokens within a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_tokenizers import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, word based tokenizers need some corpus. In this example, we will use the `tinyshakespeare` dataset. This requires the `requests` package. (`pip install requests`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get(\n",
    "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    ").text[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Byte Pair Encoding algorithm is described in the paper Sennrich et al., 2016 (<https://arxiv.org/pdf/1508.07909>). The tokenizers starts off with unicode characters and extends its vocabulary using the BPE algorithm.The algorithm finds the most frequent bigrams (pairs of tokens) in the corpus and merges them into new tokens. The new tokens are then used as the vocabulary for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit(data, vocab_size=512)\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Github.\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tokenizer also contains unicode characters, it can handle unseen words. These will just be chopped up into more tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since fitting the BPE tokenizer can take some time, the tokenizer can also be saved to disk and loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "sd = tokenizer.get_state_dict()\n",
    "with open(\"tokenizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(sd, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer.pkl\", \"rb\") as file:\n",
    "    sd = pickle.load(file)\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load_state_dict(sd)\n",
    "tokenizer.vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
